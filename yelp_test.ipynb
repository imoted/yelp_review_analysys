{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bda31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "from nltk.parse import corenlp\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41b5a3cd-bb49-4d0b-b285-e0920b137fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python --version\n",
    "# !which python\n",
    "# !jupyter --version\n",
    "# !pip -v list\n",
    "\n",
    "# !pip install jupyterlab_vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c029b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonファイルの読み込み\n",
    "\n",
    "filename = \"dataset_my-task_2021-09-16_08-10-06-950.json\"\n",
    "dir_name = \"log/\"\n",
    "sum_dir_name = \"summary/\"\n",
    "\n",
    "with open(filename, \"r\") as f:\n",
    "    json_obj = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5524c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review のtextとrating / wordのリスト化\n",
    "\n",
    "word_2d_list = []\n",
    "rating_list = []\n",
    "text_list = []\n",
    "sentense_list = []\n",
    "reviewer_loc_list = []\n",
    "reviewers_cnt_list= []\n",
    "is_funny_cnt_list = []\n",
    "is_useful_cnt_list = []\n",
    "is_cool_cnt_list = []\n",
    "date_list = []\n",
    "avoid_word_list = [\"(\", \")\", \"!\", \",\", \":\", \";\" ,\"?\", \".\", \"+\", \"--\", \"\\\"\", \"/\", \"#\"]\n",
    "\n",
    "\n",
    "for i in range(len(json_obj[0][\"reviews\"])):\n",
    "    word_list = json_obj[0][\"reviews\"][i][\"text\"].split()\n",
    "    text_list.append(json_obj[0][\"reviews\"][i][\"text\"])\n",
    "    sentense_list.append(list(re.split(\"[.\\n!]\", json_obj[0][\"reviews\"][i][\"text\"])))\n",
    "    for avoid_word in avoid_word_list:\n",
    "        word_list = [ x.replace(avoid_word, \"\").lower() for x in word_list]\n",
    "    word_list = [x for x in word_list if not x.isdigit() and x != \"\" and len(x) < 20 and len(x) != 0]\n",
    "    word_2d_list.append(word_list)\n",
    "    date_str = json_obj[0][\"reviews\"][i][\"date\"]\n",
    "    date_list.append(datetime.datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S.000Z'))\n",
    "    \n",
    "    rating_list.append(int(json_obj[0][\"reviews\"][i][\"rating\"]))\n",
    "    reviewer_loc_list.append(json_obj[0][\"reviews\"][i][\"reviewerLocation\"])\n",
    "    reviewers_cnt_list.append(json_obj[0][\"reviews\"][i][\"reviewerReviewCount\"])\n",
    "    is_cool_cnt_list.append(json_obj[0][\"reviews\"][i][\"isCoolCount\"])\n",
    "    is_funny_cnt_list.append(json_obj[0][\"reviews\"][i][\"isFunnyCount\"])\n",
    "    is_useful_cnt_list.append(json_obj[0][\"reviews\"][i][\"isUsefulCount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04b75e4-f71a-43a5-a716-3c7c1c56656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cityを集計して保存\n",
    "\n",
    "city_cnt = collections.Counter()\n",
    "city_cnt_after_200101 = collections.Counter()\n",
    "ref_date = datetime.datetime(2020, 1, 1, 0, 0, 0)\n",
    "\n",
    "for reviewer_loc, date in zip(reviewer_loc_list, date_list) :\n",
    "    city_cnt[reviewer_loc.lower()] += 1\n",
    "    if ref_date < date:\n",
    "        city_cnt_after_200101[reviewer_loc.lower()] += 1\n",
    "\n",
    "output_file_name = sum_dir_name + \"city_ranking.csv\"\n",
    "\n",
    "city_sorted = sorted(city_cnt.items(), key=lambda x:x[1], reverse=True)\n",
    "city_a_200101_sorted = sorted(city_cnt_after_200101.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "with open(output_file_name, \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"city 全区間\",\"\", \"city 20200101〜\",\"\"])\n",
    "    for city, city_after in zip(city_sorted, city_a_200101_sorted):\n",
    "        writer.writerow([city[0], city[1], city_after[0], city_after[1] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d6fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordを集計して保存\n",
    "\n",
    "word_cnt = collections.Counter()\n",
    "ref_date = datetime.datetime(2021, 7, 12, 0, 0, 0)\n",
    "\n",
    "for word_list, rating, date in zip(word_2d_list, rating_list, date_list):\n",
    "    if ref_date < date:\n",
    "        rating_val = rating - 3\n",
    "        for word in word_list:\n",
    "            word_cnt[word.lower()] += rating_val\n",
    "\n",
    "# 星毎に、星の数を集計\n",
    "\n",
    "word_dict = word_cnt.copy()\n",
    "# print(word_dict)\n",
    "for k, v in word_dict.items():\n",
    "    word_dict[k] = list([v, 0 ,0, 0, 0, 0 ])\n",
    "#     print(word_dict[k])\n",
    "\n",
    "for word_list, rating, date in zip(word_2d_list, rating_list, date_list):\n",
    "    if ref_date < date:\n",
    "        for word in word_list:\n",
    "            word_dict[word.lower()][rating ] += 1\n",
    "    #         if len(word_dict[word.lower()]) <= 6: # 例文をappend済みでなければ\n",
    "    #             word_dict[word.lower()].append(word_list)\n",
    "\n",
    "# sortしてファイルに出力\n",
    "\n",
    "output_file_name = sum_dir_name + \"output.csv\"\n",
    "\n",
    "word_dict_sorted = sorted(word_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "with open(output_file_name, \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"word\", \"review総計\" , \"星1\", \"星2\", \"星3\", \"星4\", \"星5\"])\n",
    "    for data_list in word_dict_sorted:\n",
    "        writer.writerow([ data_list[0], data_list[1][0], data_list[1][1], data_list[1][2], data_list[1][3], data_list[1][4], data_list[1][5] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c084432-9428-4287-8d9f-36169c3117a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(11):\n",
    "#     print(\"----------------------\")\n",
    "#     for i, sentense in enumerate(sentense_list[i]):\n",
    "#         print(i ,\" : \" , sentense)\n",
    "#         print(\"\")\n",
    "    # print(text_list[i])\n",
    "    # print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f0957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章のdependencyを解析\n",
    "# 長時間かかるので注意　pickle化してある。\n",
    "\n",
    "# pickle_file_name = 'dep.pickle'\n",
    "\n",
    "# dep_parser = corenlp.CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "\n",
    "# parse_list = []\n",
    "# parse_2d_list = []\n",
    "\n",
    "# def sentense_dependency():\n",
    "#     for i, sentences in tqdm.tqdm(enumerate(sentense_list)):  # 3380のレビューの数 # 1582it エラー\n",
    "#         # if i < 1581:\n",
    "#         #     continue\n",
    "#         for sentence in sentences:  # １レビューあたり入っている文章段落の数\n",
    "#             if not sentence or len(sentence) < 15:\n",
    "#                 continue\n",
    "#             # print(\"test : \", sentence)\n",
    "#             parse, = dep_parser.raw_parse(sentence)\n",
    "#             parse_tmp = parse.to_conll(4).split(\"\\n\")\n",
    "#             parse_2d_list.append([x.split(\"\\t\") for x in parse_tmp ])  # if not re.search(\"[()!,:;?.+-\\/#]\", x) if x != \"\"\n",
    "            \n",
    "# sentense_dependency()\n",
    "\n",
    "# with open(pickle_file_name, 'wb') as f:\n",
    "#     pickle.dump(parse_2d_list, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a871a706-1329-4ce2-a453-12c46a877445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['100', 'CD', '2', 'nummod'], ['%', 'NN', '3', 'nsubj'], ['recommend', 'VBP', '0', 'ROOT'], ['getting', 'VBG', '3', 'xcomp'], ['this', 'DT', '4', 'obj'], ['']]\n",
      "['100', 'CD', '2', 'nummod']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# pickle呼び出し\n",
    "\n",
    "pickle_file_name = 'dep.pickle'\n",
    "\n",
    "with open(pickle_file_name, 'rb') as f:\n",
    "    parse_2d_list = pickle.load(f)\n",
    "    \n",
    "# for parse_list in parse_2d_list: \n",
    "#     print(parse_list)\n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1e64d1-eb76-4930-a0d4-66860d5b3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['100', 'CD', '2', 'nummod'], ['%', 'NN', '3', 'nsubj'], ['recommend', 'VBP', '0', 'ROOT'], ['getting', 'VBG', '3', 'xcomp'], ['this', 'DT', '4', 'obj'], ['']]\n",
      "['100', 'CD', '2', 'nummod']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # print(parse_2d_list)\n",
    "print(parse_2d_list[10])\n",
    "print(parse_2d_list[10][0])\n",
    "print(parse_2d_list[10][0][0])\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5157480-6bca-4e6d-aff3-3af5d34959e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'SyntaxError'>",
     "evalue": "unexpected EOF while parsing (2604225226.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/xpython_29536/2604225226.py\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    # dep_cnt_list[ ]\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_name = \"dep_log/\"\n",
    "dep_cnt_list = [collections.Counter() for x in range(len(word_cnt))]\n",
    "\n",
    "print(len(parse_2d_list))\n",
    "print(len(parse_2d_list[0]))\n",
    "print(len(word_cnt.keys()))\n",
    "\n",
    "# for word_list, sentense, rating, reviewer_loc, reviewers_cnt, is_cool_cnt, is_funny_cnt, is_useful_cnt in zip(word_2d_list, sentense_list, rating_list, reviewer_loc_list, reviewers_cnt_list, is_cool_cnt_list, is_funny_cnt_list, is_useful_cnt_list ):\n",
    "for word in word_cnt.keys():\n",
    "    for parse_list in parse_2d_list:\n",
    "        for parse in parse_list:\n",
    "            # print(parse[0], word)abs #OK\n",
    "            if parse[0] == word:\n",
    "                # print(parse_list[int(parse[2])][0])\n",
    "                # dep_cnt_list[ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7aa3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review textをword毎に集計\n",
    "\n",
    "output_example_wordlist = [\"Wait\",\"Service\",\"Server\",\"Noodle\",\"Togo\",\"Broth\",\"Soup\",\"Host\",\"Appetizer\",\"Bad\",\"Worst\",\"Good\",\"Best\",\"Awesome\"]\n",
    "\n",
    "ref_date = datetime.datetime(2021, 7, 12, 0, 0, 0)\n",
    "\n",
    "for output_example in output_example_wordlist:\n",
    "    if os.path.exists(dir_name + output_example.lower() + \"_check_\" + \".txt\"):\n",
    "        continue\n",
    "    else:\n",
    "        with open(dir_name + output_example.lower() + \"_check_\" + \".txt\", \"w\") as f:\n",
    "            pass\n",
    "        for word_list, text, rating, reviewer_loc, reviewers_cnt, is_cool_cnt, is_funny_cnt, is_useful_cnt, date in zip(word_2d_list, text_list, rating_list, reviewer_loc_list, reviewers_cnt_list, is_cool_cnt_list, is_funny_cnt_list, is_useful_cnt_list, date_list ):\n",
    "        # for word_list, sentense, rating, reviewer_loc, reviewers_cnt, is_cool_cnt, is_funny_cnt, is_useful_cnt in zip(word_2d_list, sentense_list, rating_list, reviewer_loc_list, reviewers_cnt_list, is_cool_cnt_list, is_funny_cnt_list, is_useful_cnt_list ):\n",
    "            for word in word_list:\n",
    "                if ref_date < date:\n",
    "                    if word == output_example.lower():\n",
    "                        with open(dir_name + word.lower() + \"_rating_\" + str(rating) + \".txt\", \"a\") as f:\n",
    "                            f.writelines(\"date : \" + str(date) + \"\\n\")\n",
    "                            f.writelines(\"reviewer Location : \" + reviewer_loc + \"\\n\")\n",
    "                            f.writelines(\"Reviewer Count : \" + str(reviewers_cnt) + \"\\n\")\n",
    "                            f.writelines(\"is Funny Count : \" + str(is_funny_cnt) + \"\\n\")\n",
    "                            f.writelines(\"is Usuful Count : \" + str(is_useful_cnt) + \"\\n\")\n",
    "                            f.writelines(\"is Cool Count : \" + str(is_cool_cnt) + \"\\n\\n\")\n",
    "                            f.writelines(text)\n",
    "                            f.writelines(\"\\n\\n===========================================\\n\\n\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af8a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 全ワードのtext列挙\n",
    "\n",
    "# for word_d in word_dict.keys():\n",
    "#     if os.path.exists(dir_name + word_d.lower() + \"_check_\" + \".txt\"):\n",
    "#         continue\n",
    "#     else:\n",
    "#         with open(dir_name + word_d.lower() + \"_check_\" + \".txt\", \"w\") as f:\n",
    "#             pass \n",
    "#     for word_list, text, rating, reviewer_loc, reviewers_cnt, is_cool_cnt, is_funny_cnt, is_useful_cnt in zip(word_2d_list, text_list, rating_list, reviewer_loc_list, reviewers_cnt_list, is_cool_cnt_list, is_funny_cnt_list, is_useful_cnt_list ):\n",
    "#         for word in word_list:\n",
    "#             if word == word_d:\n",
    "#                 with open(dir_name + word.lower() + \"_rating_\" + str(rating) + \".txt\", \"a\") as f:\n",
    "#                     f.writelines(\"reviewer Location : \" + reviewer_loc + \"\\n\")\n",
    "#                     f.writelines(\"Reviewer Count : \" + str(reviewers_cnt) + \"\\n\")\n",
    "#                     f.writelines(\"is Funny Count : \" + str(is_funny_cnt) + \"\\n\")\n",
    "#                     f.writelines(\"is Usuful Count : \" + str(is_useful_cnt) + \"\\n\")\n",
    "#                     f.writelines(\"is Cool Count : \" + str(is_cool_cnt) + \"\\n\\n\")\n",
    "#                     f.writelines(text)\n",
    "#                     f.writelines(\"\\n\\n===========================================\\n\\n\")\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b473b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ROOT                          \n",
      "                      |                             \n",
      "                      S                            \n",
      "       _______________|__________________________   \n",
      "      |                         VP               | \n",
      "      |                _________|___             |  \n",
      "      |               |             PP           | \n",
      "      |               |     ________|___         |  \n",
      "      NP              |    |            NP       | \n",
      "  ____|__________     |    |     _______|____    |  \n",
      " DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  . \n",
      " |    |     |    |    |    |    |       |    |   |  \n",
      "The quick brown fox jumps over the     lazy dog  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = corenlp.CoreNLPParser(url='http://localhost:9000')\n",
    "next( parser.raw_parse('The quick brown fox jumps over the lazy dog.') ).pretty_print()  \n",
    "# next( parser.raw_parse(text_list[5]) ).pretty_print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f0d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8241eaf6-c5c2-4dcd-8e42-060c33c11770",
   "metadata": {},
   "source": [
    "# 以下ゴミ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d68293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency parse with CoreNLP\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# https://stanfordnlp.github.io/CoreNLP/download.html\n",
    "\n",
    "# export CLASSPATH=$CLASSPATH:/home/ted/Downloads/stanford-corenlp-4.2.2/*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b184b392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review　から、文章のlist　その下に wordのlistに分割する処理\n",
    "\n",
    "# input_review = re.split(\"[.\\n!]\", text_list[7])\n",
    "\n",
    "# input_review_devide_space = [x.split() for x in input_review if x]\n",
    "\n",
    "# out_review = []\n",
    "# for sentense in input_review_devide_space:\n",
    "#     out_sentense = []\n",
    "#     for word in sentense:\n",
    "#         re.sub(\"[(\\)\\!\\,\\:\\;\\?\\.\\+\\--\\/\\#]\", \"\", word)\n",
    "#         word = word.lower()\n",
    "#         if word:\n",
    "#             out_sentense.append(word)\n",
    "# #         print(word)\n",
    "#     if out_sentense:\n",
    "#         out_review.append(out_sentense)\n",
    "# print(input_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0fbb78-5e44-43fb-a0d7-7c0756db4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_parser = corenlp.CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "\n",
    "# for review in input_review:\n",
    "#     # print(out_s)\n",
    "#     if not review or review ==\" \":\n",
    "#         continue\n",
    "#     parse, = dep_parser.raw_parse(review)\n",
    "#     # 'The quick brown fox jumps over the lazy dog.'\n",
    "#     print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0393c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e08d84e",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/xpython_29536/1927891203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'treebank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wsj_0001.mrg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Natural Language Toolkit\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "nltk.download('treebank')\n",
    "\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[1]\n",
    "t.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f9217fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('.', '.'), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 使えない。\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning. Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c69a6c-e20a-441d-a39e-7643f048faf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(will\n",
      "  (Vinken Pierre , (old (years 61)) ,)\n",
      "  (join (board the) (as (director a nonexecutive)) (Nov. 29) .))\n"
     ]
    }
   ],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import (DependencyGraph,ProjectiveDependencyParser,NonprojectiveDependencyParser)\n",
    "treebank_data = \"\"\"Pierre  NNP     2       NMOD\n",
    "Vinken  NNP     8       SUB\n",
    ",       ,       2       P\n",
    "61      CD      5       NMOD\n",
    "years   NNS     6       AMOD\n",
    "old     JJ      2       NMOD\n",
    ",       ,       2       P\n",
    "will    MD      0       ROOT\n",
    "join    VB      8       VC\n",
    "the     DT      11      NMOD\n",
    "board   NN      9       OBJ\n",
    "as      IN      9       VMOD\n",
    "a       DT      15      NMOD\n",
    "nonexecutive    JJ      15      NMOD\n",
    "director        NN      12      PMOD\n",
    "Nov.    NNP     9       VMOD\n",
    "29      CD      16      NMOD\n",
    ".       .       9       VMOD\"\"\"\n",
    "\n",
    "dg = DependencyGraph(treebank_data)\n",
    "dg.tree().pprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
